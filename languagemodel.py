# -*- coding: utf-8 -*-
"""LanguageModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h5BSfOlgT-8P-lVvjpPih8gQb4dLHE-h
"""

import sys
print("Assignment 2: Neural Language Model (PyTorch) â€” Notebook Start")
print("Python:", sys.version.splitlines()[0])


import torch
import numpy as np
import random
print("PyTorch version:", torch.__version__)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

import os, time, math, json, itertools
from pathlib import Path
import matplotlib.pyplot as plt
from collections import Counter, defaultdict

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

DATA_PATH = Path("data")
DATA_PATH.mkdir(exist_ok=True)
print("Place the provided dataset (plain text) in:", DATA_PATH.resolve())
print("Expected: one text file (utf-8). If you have a dataset folder, set TEXT_FILE variable below.")
TEXT_FILE = DATA_PATH / "dataset.txt"
print("Current TEXT_FILE =", TEXT_FILE)

from google.colab import files

print("ðŸ“¤ Please upload your dataset file (e.g., .txt)...")
uploaded = files.upload()

if len(uploaded) == 0:
    print("No file uploaded. Please re-run this cell and upload a text file.")
    raw_text = None
else:
    filename = list(uploaded.keys())[0]
    print(f"âœ… File '{filename}' uploaded successfully.")
    with open(filename, "r", encoding="utf-8", errors="ignore") as f:
        raw_text = f.read()
    print("Loaded file size (chars):", len(raw_text))
    n_lines = raw_text.count("\n") + 1
    print("Approx lines:", n_lines)
    print("\nFirst 500 characters:\n")
    print(raw_text[:500])

class Tokenizer:
    def __init__(self, min_freq=1, unk_token="<unk>", pad_token="<pad>", bos_token="<bos>", eos_token="<eos>"):
        self.min_freq = min_freq
        self.unk_token = unk_token
        self.pad_token = pad_token
        self.bos_token = bos_token
        self.eos_token = eos_token
        self.token_to_id = {}
        self.id_to_token = {}
        self.fitted = False

    @staticmethod
    def simple_tokenize(text):
        return text.strip().split()

    def fit(self, texts):
        counter = Counter()
        for t in texts:
            counter.update(self.simple_tokenize(t))
        vocab = [self.pad_token, self.unk_token, self.bos_token, self.eos_token]
        for tok, freq in counter.most_common():
            if freq >= self.min_freq:
                vocab.append(tok)
        self.token_to_id = {tok: idx for idx, tok in enumerate(vocab)}
        self.id_to_token = {idx: tok for tok, idx in self.token_to_id.items()}
        self.fitted = True
        print(f"Vocab size: {len(self.token_to_id)} (min_freq={self.min_freq})")
        return self

    def encode(self, text):
        if not self.fitted:
            raise RuntimeError("Tokenizer not fitted")
        toks = self.simple_tokenize(text)
        ids = [self.token_to_id.get(tok, self.token_to_id[self.unk_token]) for tok in toks]
        return ids

    def decode(self, ids):
        return " ".join(self.id_to_token.get(i, self.unk_token) for i in ids)

if raw_text is None:
    print("No raw_text loaded; abort tokenization step.")
else:
    lines = [ln.strip() for ln in raw_text.splitlines() if ln.strip()]
    print("Effective text units:", len(lines))

    tokenizer = Tokenizer(min_freq=1)
    tokenizer.fit(lines)

    tokenized_lines = [tokenizer.encode(ln) for ln in lines]

    stream = []
    for seq in tokenized_lines:
        stream.append(tokenizer.token_to_id[tokenizer.bos_token])
        stream.extend(seq)
        stream.append(tokenizer.token_to_id[tokenizer.eos_token])
    print("Total tokens in stream:", len(stream))

class LangModelDataset(Dataset):
    def __init__(self, token_stream, seq_len):
        self.seq_len = seq_len
        self.stream = token_stream
        self.examples = []
        total = len(token_stream)
        for i in range(total - seq_len):
            x = token_stream[i:i+seq_len]
            y = token_stream[i+1:i+seq_len+1]
            self.examples.append((x, y))
        print(f"Created {len(self.examples)} examples (seq_len={seq_len})")

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        x, y = self.examples[idx]
        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)

def collate_batch(batch):
    xs, ys = zip(*batch)
    x = torch.stack(xs)
    y = torch.stack(ys)
    return x, y

if raw_text is not None:
    total_tokens = len(stream)
    split_at = int(0.9 * total_tokens)
    train_stream = stream[:split_at]
    val_stream = stream[split_at:]
    print("Train tokens:", len(train_stream), "Val tokens:", len(val_stream))
    DEFAULT_SEQ_LEN = 30

    train_ds_tmp = LangModelDataset(train_stream, seq_len=DEFAULT_SEQ_LEN)
    val_ds_tmp = LangModelDataset(val_stream, seq_len=DEFAULT_SEQ_LEN)

class LSTMLanguageModel(nn.Module):
    def __init__(self, vocab_size, emb_size=128, hidden_size=256, num_layers=1, dropout=0.1, tie_weights=False):
        super().__init__()
        self.vocab_size = vocab_size
        self.emb_size = emb_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, vocab_size)

        if tie_weights:
            if hidden_size != emb_size:
                raise ValueError("For weight tying, embedding dim must equal hidden size")
            self.fc.weight = self.embedding.weight

    def forward(self, x, hidden=None):
        emb = self.embedding(x)
        out, hidden = self.lstm(emb, hidden)
        out = self.dropout(out)
        logits = self.fc(out)
        return logits, hidden

    def init_hidden(self, batch_size, device):
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)
        return (h0, c0)

criterion = nn.CrossEntropyLoss(ignore_index=-100)


def compute_perplexity(loss):
    return math.exp(loss)

def sequence_loss_and_perplexity(logits, targets):
    vocab = logits.size(-1)
    logits_flat = logits.view(-1, vocab)
    targets_flat = targets.view(-1)
    loss = criterion(logits_flat, targets_flat)
    ppl = compute_perplexity(loss.item())
    return loss, ppl

def train_one_epoch(model, dataloader, optimizer, device, clip_grad=None, grad_accum_steps=1):
    model.train()
    total_loss = 0.0
    total_tokens = 0
    hidden = None
    for i, (x, y) in enumerate(dataloader):
        x = x.to(device)
        y = y.to(device)
        batch_size = x.size(0)

        optimizer.zero_grad()
        logits, hidden = model(x, hidden=None)
        loss, _ = sequence_loss_and_perplexity(logits, y)
        loss.backward()
        if clip_grad is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
        optimizer.step()

        total_loss += loss.item() * (x.numel() / x.size(-1))
        total_tokens += x.numel()
        hidden = None

    avg_loss = total_loss / total_tokens * dataloader.dataset.seq_len
    avg_loss = total_loss / (len(dataloader) * dataloader.dataset.seq_len)
    return avg_loss

def evaluate(model, dataloader, device):
    model.eval()
    total_loss = 0.0
    total_tokens = 0
    with torch.no_grad():
        for x, y in dataloader:
            x = x.to(device); y = y.to(device)
            logits, _ = model(x)
            loss, _ = sequence_loss_and_perplexity(logits, y)
            total_loss += loss.item() * x.size(0)
            total_tokens += x.numel()
    avg_loss = total_loss / (len(dataloader) * dataloader.dataset.seq_len)
    ppl = compute_perplexity(avg_loss)
    return avg_loss, ppl

def run_experiment(config, train_stream, val_stream, tokenizer, device):
    print("Experiment config:", json.dumps(config, indent=2))
    seq_len = config.get("seq_len", DEFAULT_SEQ_LEN)
    train_ds = LangModelDataset(train_stream, seq_len=seq_len)
    val_ds = LangModelDataset(val_stream, seq_len=seq_len)
    train_dl = DataLoader(train_ds, batch_size=config["batch_size"], shuffle=True, collate_fn=collate_batch)
    val_dl = DataLoader(val_ds, batch_size=config["batch_size"], shuffle=False, collate_fn=collate_batch)

    vocab_size = len(tokenizer.token_to_id)
    model = LSTMLanguageModel(vocab_size=vocab_size,
                              emb_size=config.get("emb_size", 128),
                              hidden_size=config.get("hidden_size", 256),
                              num_layers=config.get("num_layers", 1),
                              dropout=config.get("dropout", 0.1),
                              tie_weights=config.get("tie_weights", False)).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=config.get("lr", 1e-3), weight_decay=config.get("weight_decay", 0.0))

    best_val_loss = float("inf")
    history = {"train_loss": [], "val_loss": [], "val_ppl": []}
    for epoch in range(1, config["epochs"] + 1):
        start = time.time()
        train_loss = train_one_epoch(model, train_dl, optimizer, device, clip_grad=config.get("clip_grad", None))
        val_loss, val_ppl = evaluate(model, val_dl, device)
        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)
        history["val_ppl"].append(val_ppl)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state = model.state_dict()

        print(f"Epoch {epoch}/{config['epochs']}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_ppl={val_ppl:.2f}, time={time.time()-start:.1f}s")
    result = {"config": config, "model_state": best_state, "history": history, "vocab": tokenizer.token_to_id}
    return result

underfit_cfg = {
    "name": "underfit",
    "seq_len": 20,
    "emb_size": 32,
    "hidden_size": 32,
    "num_layers": 1,
    "dropout": 0.5,
    "tie_weights": False,
    "batch_size": 128,
    "lr": 5e-3,
    "weight_decay": 0.0,
    "epochs": 6,
    "clip_grad": 1.0
}

overfit_cfg = {
    "name": "overfit",
    "seq_len": 30,
    "emb_size": 256,
    "hidden_size": 512,
    "num_layers": 2,
    "dropout": 0.0,
    "tie_weights": False,
    "batch_size": 16,
    "lr": 1e-3,
    "weight_decay": 0.0,
    "epochs": 20,
    "clip_grad": None
}

best_fit_cfg = {
    "name": "best_fit",
    "seq_len": 30,
    "emb_size": 128,
    "hidden_size": 256,
    "num_layers": 2,
    "dropout": 0.2,
    "tie_weights": False,
    "batch_size": 64,
    "lr": 1e-3,
    "weight_decay": 1e-5,
    "epochs": 12,
    "clip_grad": 1.0
}

configs = [underfit_cfg, overfit_cfg, best_fit_cfg]

results = {}
for cfg in configs:
    print("\n\n=== Running:", cfg["name"], "===\n")
    res = run_experiment(cfg, train_stream, val_stream, tokenizer, device)
    results[cfg["name"]] = res
    out_dir = Path("trained_models")
    out_dir.mkdir(exist_ok=True)
    model_path = out_dir / f"lm_{cfg['name']}.pt"
    torch.save({"state_dict": res["model_state"], "vocab": res["vocab"], "config": cfg}, model_path)
    print("Saved best model for", cfg["name"], "to", model_path)

plt.figure(figsize=(12, 4))
for name, res in results.items():
    hist = res["history"]
    epochs = range(1, len(hist["train_loss"]) + 1)
    plt.plot(epochs, hist["train_loss"], label=f"{name} train")
    plt.plot(epochs, hist["val_loss"], "--", label=f"{name} val")
plt.xlabel("Epoch")
plt.ylabel("Loss (avg)")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(8,4))
for name, res in results.items():
    plt.plot(range(1, len(res["history"]["val_ppl"]) + 1), res["history"]["val_ppl"], label=name)
plt.xlabel("Epoch")
plt.ylabel("Validation Perplexity")
plt.title("Validation Perplexity")
plt.legend()
plt.grid(True)
plt.show()

import shutil
from google.colab import files

shutil.make_archive("trained_models", "zip", "trained_models")
print("ðŸ“¦ created trained_models.zip")

files.download("trained_models.zip")

for name, res in results.items():
    final_epoch = len(res["history"]["val_ppl"])
    final_ppl = res["history"]["val_ppl"][-1]
    best_ppl = min(res["history"]["val_ppl"])
    best_epoch = res["history"]["val_ppl"].index(best_ppl) + 1
    print(f"{name}: final val PPL={final_ppl:.2f}, best val PPL={best_ppl:.2f} (epoch {best_epoch})")

def load_model_from_result(res, device):
    cfg = res["config"]
    vocab = res["vocab"]
    inv_vocab = {i:t for t,i in vocab.items()}
    model = LSTMLanguageModel(vocab_size=len(vocab),
                              emb_size=cfg.get("emb_size",128),
                              hidden_size=cfg.get("hidden_size",256),
                              num_layers=cfg.get("num_layers",1),
                              dropout=cfg.get("dropout",0.1)).to(device)
    model.load_state_dict(res["model_state"])
    model.eval()
    return model, vocab, inv_vocab

def generate_text(model, vocab, inv_vocab, tokenizer, seed_text, max_len=30, device=device):
    model.eval()
    ids = [tokenizer.token_to_id.get(tokenizer.bos_token)]
    ids += tokenizer.encode(seed_text)
    input_ids = torch.tensor([ids[-config_seq_len:]], dtype=torch.long).to(device) if (config_seq_len := 30) else torch.tensor([ids], dtype=torch.long).to(device)
    generated = ids.copy()
    with torch.no_grad():
        for _ in range(max_len):
            x = torch.tensor([generated[-30:]], dtype=torch.long).to(device)
            logits, _ = model(x)
            next_logits = logits[0, -1, :]
            next_id = torch.argmax(next_logits).item()
            generated.append(next_id)
            if inv_vocab.get(next_id) == tokenizer.eos_token:
                break
    return tokenizer.decode(generated)

best_name = "best_fit" if "best_fit" in results else list(results.keys())[0]
model_obj, vocab_map, inv_vocab_map = load_model_from_result(results[best_name], device)
sample = "This"
print("Generated (greedy) starting with 'This':")
print(generate_text(model_obj, vocab_map, inv_vocab_map, tokenizer, sample, max_len=40))