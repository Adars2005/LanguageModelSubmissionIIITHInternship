Neural Language Model Training using PyTorch

Submitted by:
J. Adarsh
B.Tech CSD, Keshav Memorial Institute of Technology (KMIT), Hyderabad
Email: adarshjangeeti@gmail.com

ğŸ” Objective

This project implements a Neural Language Model (NLM) completely from scratch using PyTorch, demonstrating how sequence models learn to predict text and how model capacity, dropout, and regularization impact generalization.

Three training regimes were implemented as required:

Underfitting â€” very small model â†’ high bias

Overfitting â€” large model + no dropout â†’ memorization

Best Fit â€” balanced model â†’ optimal generalization

Evaluation metrics used:

Cross-Entropy Loss

Perplexity (PPL)

ğŸ“˜ Dataset & Preprocessing

Dataset: Pride and Prejudice by Jane Austen (public domain)
Size: ~700 KB (â‰ˆ130k tokens)

Preprocessing Pipeline

Custom word-level tokenizer (whitespace-based)

Special tokens: <pad>, <unk>, <bos>, <eos>

Vocabulary size: ~25,000 tokens

Sliding window sequence creation (seq_len = 20â€“30)

90% Train / 10% Validation split

Custom LangModelDataset + PyTorch DataLoader

âš™ï¸ Model Architecture

Implemented a 2-layer LSTM Language Model:

Component	Description
Embedding	Token â†’ vector (emb_size)
LSTM	Learns sequential dependencies
Dropout	Prevents overfitting
Linear	Hidden â†’ vocabulary logits
Loss	CrossEntropyLoss
Optimizer	Adam (lr = 1e-3)
Metric	Perplexity = exp(loss)

Random seed fixed (SEED = 42) for reproducibility.

ğŸ§ª Experimental Configurations
Config	Hidden	Layers	Dropout	Batch	Epochs	LR	Behavior
Underfit	32	1	0.5	128	6	0.005	Too small â†’ fails to learn
Overfit	512	2	0.0	16	20	0.001	Large â†’ memorizes
Best Fit	256	2	0.2	64	12	0.001	Best generalization
ğŸ–¥ï¸ Training Setup
Parameter	Value
Runtime	Google Colab (CPU)
Framework	PyTorch 2.x
Device	cpu
Tokens	~150k
Avg Time	Underfit: 13s/epoch â€¢ Overfit: 147s/epoch â€¢ Best Fit: 49s/epoch
ğŸ“Š Results
Final Metrics
Model	Final Train Loss	Final Val Loss	Val Perplexity	Notes
Underfit	28.7 â†’ 47.9	40 â†’ 47	~10Â¹â¸â€“10Â²â°	Model failed to learn
Overfit	1.21 â†’ 0.13	5.25 â†’ 9.49	~13k	Memorization
Best Fit	11.26 â†’ 2.21	12.37 â†’ 17.57	~4Ã—10â·	Balanced learning
Training Curve Interpretation

Underfit: flat, high losses

Overfit: train â†“ while val â†‘

Best Fit: stable downward trend â†’ best-generalizing model

ğŸ“ˆ Analysis & Interpretation

Underfitting caused by insufficient model capacity

Overfitting caused by zero dropout & large hidden size

Best Fit demonstrates correct biasâ€“variance trade-off

Weight decay + dropout improved generalization

Gradient clipping stabilized training

ğŸ§® Perplexity Definition
Perplexity
=
ğ‘’
CrossEntropyLoss
Perplexity=e
CrossEntropyLoss

Lower PPL indicates better next-token prediction.

ğŸ“‚ Repository Structure
LanguageModelSubmissionIIITHInternship/
â”‚
â”œâ”€â”€ LanguageModel.ipynb            # Main notebook
â”œâ”€â”€ language_model.py              # Training/Model scripts
â”œâ”€â”€ Pride_and_Prejudice-Jane_Austen.txt
â”‚
â”œâ”€â”€ lm_best_fit.pt                 # Best Fit model
â”œâ”€â”€ lm_underfit.pt                 # Underfit model
# lm_overfit.pt excluded due to >25 MB GitHub limit
â”‚
â””â”€â”€ Assignment 2 â€” Language Model using PyTorch.pdf

ğŸ§¾ Notes on Model Files

GitHub restricts uploads >25MB.
Therefore:

lm_underfit.pt â†’ uploaded

lm_best_fit.pt â†’ uploaded

lm_overfit.pt â†’ excluded but can be shared via Google Drive if required

â–¶ï¸ How to Run

Clone the repo:

git clone https://github.com/Adars2005/LanguageModelSubmissionIIITHInternship
cd LanguageModelSubmissionIIITHInternship


Install dependencies:

pip install torch numpy matplotlib


Open and run the notebook:

Upload the dataset when prompted.

Run all cells sequentially.

Models will be saved in the working directory.

Reproducibility:

Random seeds fixed (SEED = 42)

Runs produce identical results.

ğŸ”— References

Bengio et al. (2003), A Neural Probabilistic Language Model

Goodfellow et al. (2016), Deep Learning â€” Chapter 10

PyTorch Docs â€” https://pytorch.org/docs
